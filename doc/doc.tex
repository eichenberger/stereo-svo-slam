\documentclass[11pt,a4paper,titlepage,oneside]{report}
\usepackage{titling}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{lmodern}
\usepackage{amsmath}
\usepackage{float}
\usepackage{subfig}
\usepackage{listings}
\usepackage[hidelinks]{hyperref}

\input{layout}

\setlength{\parindent}{0pt}

\title{ORB Slam Point Cloud generation on Apalis iMX8}
\author{Stefan Eichenberger}
\date{February 2019}
\advisors{Marcus Hudritsch}
\department{TSM CPVR Lab}

\lstset{
	basicstyle=\ttfamily\scriptsize
}

\begin{document}
\title{Open source SLAM library for embedded systems}

\maketitle
\begin{abstract}
  Simultaneous location and mapping (SLAM) is a technology used for robot navigation and augmented reality. Today most SLAM libraries are either proprietary or not meant for embedded systems. In this thesis we write a library which is open source and achieves frame rates of above 30 fps when running on embedded devices.
\end{abstract}

\tableofcontents

\chapter{Planing}

This chapter describes the planing of the master thesis. The first section lists the requirements the second section shows the time planing.

\section{Requirement Specification}

This section lists the features that must and shall be implemented during the master thesis. The word must means that these are hard requirements. Shall describes nice to have features that aren't hardly required. Can are options that are not required at all. The numbers after \# are IDs found in the Gantt chart of figure \ref{fig:gantt}.

\subsection{SLAM Library \#42}
A SLAM library must be written which runs at 30 fps on stereo gray scale images at a resolution of 640x480 on two Intel i5-7Y54 CPUs.
\subsubsection{SVO based algorithm \#44}
A first implementation of the library shall be based on SVO. However, it's not mandatory to be the exact SVO algorithm.
\subsubsection{Plane Detection \#45}
The library must provide a mechanism to detect points laying on a plane and to remove redundant points.
\subsubsection{Plane Mesh Creation \#46}
The library must create a plane mesh from the sparse point cloud. The mesh shall contain textures from corresponding images.
\subsubsection{IMU Integration \#47}
The library must provide an interface to feed data from an IMU to estimate the motion model.
\subsubsection{Relocation \#48}
The library shall provide an interface to find the current location of the camera based on a pre-generated map
\subsubsection{Language}
The library must provide a Python 3 interface.
\subsubsection{CPU}
The library must run on x86/amd64 but shall not be limited to this architecture.
\subsection{OpenCL}
OpenCL can be used to increase the performance of the library
\subsection{Example Application \#43}
An application must be written that shows the capabilities of the library. 
\subsubsection{Mapping}
The application must create a sparse map of a typical room with four walls, one door and two windows. However, it shall not be limited to such an environment.
\subsubsection{Exporting}
The application shall have a feature to export a generated map for further use.
\subsubsection{Relocation}
The application shall have a mechanism to load a pre-generated map that can be used to do relocation.

\section{Planning}
The master thesis is honored with 27 ECTS. 1 ECTS consumes 30h which results in a total of 810h. Assuming 8.5 working hours per day, we have to spend 95.29 working days. Because the thesis is done part time a full year can be used. We assume one year has 48 weeks. During the first 16 weeks only 1.5 days can be used for the master thesis (because of additional modules). This means during the first 16 weeks we only work 24 days on the project. 71.29 days are left for the last 32 weeks. This results in 2.228 days work during the 32 weeks. Figure \ref{fig:gantt} shows the initial planing. The first implementation of the algorithm takes longer than the rest because only 1.5 days can be spent for the thesis during this period.

\begin{figure}[H]
	\includegraphics[width=1.0\textwidth]{img/gantt.png}
	\caption{Initial planing}\label{fig:gantt}
\end{figure}

\chapter{Algorithm}
In this chapter we analyse an algorithm that can be used to solve the SLAM problem.

\section{ORB SLAM}

\section{SVO SLAM}
While ORB SLAM is well documented and can do loop closing, it is difficult to optimize the algorithm for realtime applications.

\subsection{Initialization}\label{sec:initialization}
In this project we use a stereo camera therefore the initialization step is simpler than in the monocular case. The algorithm performs the following steps.
\begin{enumerate}
	\item{Search FAST keypoints on the whole image}
	\item{Do Sobel filtering in horizontal direction}
	\item{Divide image in 16x16 blocks}
	\item{Select one FAST corner in each block}
		\subitem{If no FAST corner can be found, use point with highest gradient as keypoint}
	\item{Calculate depth fore each keypoint}
		\subitem{Find the smallest intensity difference along the epipolar line}
		\subitem{The difference between these two points is the disparity (1/depth)}
\end{enumerate}

This step is described in \cite{svo2} section IX.A. We use a multi camera approach. We need to do the initialization step when we get the first frame as well as when we insert a new keyframe.

\subsection{Sparse Image Alignment}

From the initialization step we optain a initial point cloud. We use this point cloud to estimate the pose and motion of the camera. We try to find a Pose that minimizes the photometric error of small patches around the keypoints found in section \ref{sec:initialization}. We do that by minimizing the formula shown in equation \ref{eq:intensity}. We use equation \ref{eq:cm} to project the 3D point cloud to the current frame. By using Levenberg-Marquardt we can minimize the intensity difference by changing the values of the extrinsic matrix $r_{ij}$ and $t_{x,y,z}$. As initial guess for the pose we use the current pose and add the motion model. The motion model is the difference between the pose in the previous frame ($t-1$) and frame $t-2$.

\begin{equation}\label{eq:intensity}
	I_{TD}=\sum_{i=0}^{i=N}I_{F-1}(x_{F-1,i},y_{F-1,i})-I_{F}(x_{F,i},y_{F,i})
\end{equation}
Where:
\begin{align*}
	I_{TD} &:					\text{Total intensity difference over all keypoints}\\
	I_{F-1} &:				\text{Previous grayscale image}\\
	I_{F} &:					\text{Current grayscale image}\\
	x_{KF},y_{KF} &:	\text{x,y position in keyframe}\\
	N &:							\text{Total number of keypoints}\\
	x_{F},y_{F} &:		\text{x,y posiiton in current frame}
\end{align*}

\begin{equation}\label{eq:cm}
  \begin{pmatrix}
		f_x & \gamma & c_x \\
		0 & f_y & c_y \\
		0 & 0 & 1 \\
	\end{pmatrix}*
	\begin{pmatrix}
		r_{11} & r_{12} & r_{13} & t_x \\
		r_{21} & r_{22} & r_{23} & t_y \\
		r_{31} & r_{32} & r_{33} & t_z \\
	\end{pmatrix}
	\begin{pmatrix}
		X \\
		Y \\
		Z \\
		1
	\end{pmatrix}=
	\begin{pmatrix}
		u \\
		v \\
		s
  \end{pmatrix}
\end{equation}
\begin{equation}\label{eq:cm_normalized}
	\begin{pmatrix}
		x \\
		y
	\end{pmatrix}=
	\begin{pmatrix}
		u/s \\
		v/s 
  \end{pmatrix}
\end{equation}
Where:
\begin{align*}
  X,Y,Z			&: \text{point in the 3D world}\\
	u,v,s	   	&: \text{point in 2D image not normalize}\\
	x,y				&: \text{point in 2D image normalized with s}\\
	f_x,f_y  	&: \text{focal length of the camera}\\
  c_x,c_y  	&: \text{principal point}\\
  t_x,t_y,t_z	&: \text{location of the camera}\\
  r_{ij}	&: \text{part of the rotation matrix}
\end{align*}

This step is described in \cite{svo2} in section IV.A. We get a first estimate for the pose. However, because we estimate the current pose by using the previous frame we will get a drift in the long term. Therefore, we need to refine the pose by using the keyframe as reference.\\
If less than 80\% of the keypoints are trackable we need to insert a new keyframe based on the previous frame.

\subsection{Refinement}

The refinement step is necessary to reduce the drift over time by taking the information of the keyframe into account. We use the Lucas-Kanade algorithm to find an Affine transformation Matrix that minimizes the photometric error between all keypoints of the keyframe and the current frame. This will give us a reprojection error. We try to minimize this reprojection error by do ing a bundle adjustment over the pose (motion only) and over the 3D point cloud (structure only).\\
By using Lucas-Kanade we try to find the following Affine transformation:\\
\begin{equation}
	\begin{pmatrix}
		x'\\
		y'\\
	\end{pmatrix}=
	\begin{pmatrix}

\end{equation}
This step is described in \cite{svo2} in section IV.B.

\begin{thebibliography}{1}

	\bibitem{svo}
	Christian Forster et al\\
	\textit{SVO: Semi-Direct Visual Odometry for Monocular and Multi-Camera Systems}\\
	doi:10.1109/TRO.2016.2623335

	\bibitem{svo2}
	Christian Forster et al\\
	\textit{SVO: Semi-Direct Visual Odometry for Monocular and Multi-Camera Systems}\\
	doi:10.1109/TRO.2016.2623335

\end{thebibliography}

\end{document}
