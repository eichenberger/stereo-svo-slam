\documentclass[11pt,a4paper,titlepage,oneside]{report}
\usepackage{titling}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{lmodern}
\usepackage{amsmath}
\usepackage{float}
\usepackage{subfig}
\usepackage{listings}
\usepackage[hidelinks]{hyperref}

\input{layout}

\setlength{\parindent}{0pt}

\title{ORB Slam Point Cloud generation on Apalis iMX8}
\author{Stefan Eichenberger}
\date{February 2019}
\advisors{Marcus Hudritsch}
\department{TSM CPVR Lab}

\lstset{
	basicstyle=\ttfamily\scriptsize
}

\begin{document}
\title{Open source SLAM library for embedded systems}

\maketitle
\begin{abstract}
  Simultaneous location and mapping (SLAM) is a technology used for robot navigation and augmented reality. Today most SLAM libraries are either proprietary or not meant for embedded systems. In this thesis we write a library which is open source and achieves frame rates of above 30 fps when running on embedded devices.
\end{abstract}

\tableofcontents

\chapter{Introduction}

Humans have different sensors to know the pose of the head inside a room. Everyone who ever tried to stand on one foot and closed the eyes knows that this makes finding the balance harder. From this we can guess that our eyes are an import factor of finding the pose for our brain. In this thesis we try to find a way to use the same optical information from a camera to estimate the pose of the camera inside a room. This is called visual odometry (VO). In this documentation we analyse and reimplement a algorithm which is called Semi-Dens Visual Odometry (SVO). The corresponding paper states that this algorithm is fast and can be used on embedded systems. We try to reimplement this algorithm and use it together with stereo cameras while the original algorithm uses a monocular camera.\\
Even thought SVO SLAM is open source, the reference implementation is over 4 years old and has a lot of 3th party libraries. There exists a newer version called SVO2. However, this implementation is closed source. The new library written during this project will be completely independent of the original implementation. It will have less dependencies and will run faster. While the original implementation can be used with monocular camera the new library requires stereo cameras.

\section{Requirement Specification}

This section lists the features that must and shall be implemented during the master thesis. The word must means that these are hard requirements. Shall describes nice to have features that aren't hardly required. Can are options that are not required at all. The numbers after \# are IDs found in the Gantt chart of figure \ref{fig:gantt}.

\subsection{SLAM Library \#42}
A SLAM library must be written which runs at 30 fps on stereo gray scale images at a resolution of 752x480 on two Intel i5-7Y54 CPUs.
\subsubsection{SVO based algorithm \#44}
A first implementation of the library shall be based on SVO. However, it's not mandatory to be the exact SVO algorithm.
\subsubsection{Plane Detection \#45}
The library must provide a mechanism to detect points laying on a plane and to remove redundant points.
\subsubsection{Plane Mesh Creation \#46}
The library must create a plane mesh from the sparse point cloud. The mesh shall contain textures from corresponding images.
\subsubsection{IMU Integration \#47}
The library must provide an interface to feed data from an IMU to estimate the motion model.
\subsubsection{Relocation \#48}
The library shall provide an interface to find the current location of the camera based on a pre-generated map
\subsubsection{Language}
The library must provide a Python 3 interface.
\subsubsection{CPU}
The library must run on x86/amd64 but shall not be limited to this architecture.
\subsection{OpenCL}
OpenCL can be used to increase the performance of the library
\subsection{Example Application \#43}
An application must be written that shows the capabilities of the library. 
\subsubsection{Mapping}
The application must create a sparse map of a typical room with four walls, one door and two windows. However, it shall not be limited to such an environment.
\subsubsection{Exporting}
The application shall have a feature to export a generated map for further use.
\subsubsection{Relocation}
The application shall have a mechanism to load a pre-generated map that can be used to do relocation.

\section{Planning}
The master thesis is honored with 27 ECTS. 1 ECTS consumes 30h which results in a total of 810h. Assuming 8.5 working hours per day, we have to spend 95.29 working days. Because the thesis is done part time a full year can be used. We assume one year has 48 weeks. During the first 16 weeks only 1.5 days can be used for the master thesis (because of additional modules). This means during the first 16 weeks we only work 24 days on the project. 71.29 days are left for the last 32 weeks. This results in 2.228 days work during the 32 weeks. Figure \ref{fig:gantt} shows the initial planing. The first implementation of the algorithm takes longer than the rest because only 1.5 days can be spent for the thesis during this period.

\begin{figure}[H]
	\includegraphics[width=1.0\textwidth]{img/gantt.png}
	\caption{Initial planing}\label{fig:gantt}
\end{figure}

\section{Final timeline}

Unfortunately not all requirements are available in the final reason. As we will see the main reason for this is that implementing the algorithm took more time than expected. The final version now includes the following features:
\begin{itemize}
	\item{SLAM library \#42}
		\subitem{30 fps are not achieved on an i5-7Y54 but on Quad Core i7 processor}
	\item{SVO based algorithm \#44}
	\item{IMU Integration \#47}
	\item{Language}
	\item{CPU}
	\item{Example Application \#43}
\end{itemize}

\chapter{SLAM Evaluation}\label{chap:evaluation}

In this section we analyze different documented SLAMs. This work was not part of this thesis. However, it shows what other algorithms and possibilities are available today. We can split them in two groups, indirect and direct SLAM (figure \ref{fig:slammodes}). Indirect methods analyze an image and try to extract features. This feature points are matched directly to further images. Based on these matches we can estimate the pose of the camera. Direct methods operate on intensity variances. The goal is to find a pose that minimizes the intensity difference between two images. Because minimizing the intensity difference over the whole image is computational expensive most methods operated only on edges or corners of the image. We call such methods direct semi dense or direct sparse. In the next sections we will have a discussion about these different approaches.

\begin{figure}[H]
	\includegraphics[width=1.0\textwidth]{img/slam_modes.png}
	\caption{SLAM Modes}\label{fig:slammodes}
\end{figure}


\section{Indirect method}

A well documented indirect method is ORB SLAM \cite{orbslam}. ORB SLAM extracts ORB keypoints which are then matched with a known 3D point cloud. Based on these matches it can do PnP with RANSAC \cite{ransac} to estimate the current pose and position. Indirect methods can estimate the pose through PnP which isn’t computationally expensive. On the other side computing descriptors costs CPU time. ORB was explained in greater detail in the project work 2 of this thesis TODO cite.

\section{Direct method}

In this section we will analyze two direct approaches a dense and a sparse approach. Dense methods use all pixels on the image for tracking while sparse methods only use a subset. Figure \ref{fig:sparse_dense} shows a comparison of dense sparse and semi-dense approaches. We don't dig into semi-dense approaches they work similar to sparse methods but with more keypoints.

\begin{figure}[H]
  \begin{center}
		\includegraphics[width=1.0\textwidth]{img/sparse_dense.png}
  \end{center}
	\caption{Comparison of sparse, dense and semi-dense approaches \cite{svo}}\label{fig:sparse_dense}
\end{figure}

\subsection{Dense}

There are not that many dense SLAM algorithms. One is DTAM \cite{dtam} which uses the intensity values of the whole image to estimating the pose. The idea is to minimize the intensity difference (energy) between two images by optimizing the camera pose. The energy defined at one pixel position is defined as shown in equation \ref{eq:pixel_energy}. To estimate the pose and position it tries to find a projection matrix that minimizes $E_{t}$ as shown in equation \ref{eq:total_energy}.
\begin{equation}\label{eq:pixel_energy}
\begin{aligned}
  E_{xy}=I_1(x,y)-I_2(x',y')\\
	\begin{pmatrix}
	x'*s'\\
	y'*s'\\
	s'\\
	\end{pmatrix}=P*XYZ
\end{aligned}
\end{equation}
Where:
\begin{align*}
  E_{xy}	&: \text{Energy at a specific position}\\
  I_1			&: \text{Previous image}\\
  I_2			&: \text{Current image}\\
  x,y			&: \text{Point position in previous image} \\
  x',y'		&: \text{Point position in current image} \\
	s'			&: \text{Scaling value}\\
  P				&: \text{Projection matrix}\\
  XYZ			&: \text{Point position in 3D cloud}
\end{align*}
\begin{equation}\label{eq:total_energy}
  E_{t}=\min(\sum_{x=0}^C\sum_{y=0}^RE_{xy})
\end{equation}
Where:
\begin{align*}
  E_{t}		&: \text{Total energy}\\
  E_{xy}	&: \text{Energy at pixel position x/y in image}\\
	C				&: \text{Image columns}\\
	R				&: \text{Image rows}\\
\end{align*}

Direct dense methods deliver a dense point cloud which we can use directly to create maps and 3D objects. However, they are computational expensive. Therefore, they are not appropriate for embedded devices.

\subsection{Sparse}

Sparse direct SLAMs like SVO \cite{svo} don’t optimize the Energy over the whole image but the energy at sparse points instead. One possibility of a sparse point is e.g. a corner point found by FAST \cite{fast}. Similar to sparse methods there are semi dense methods which try to minimize the energy on several points laying on edges. Canny Edge detection or DoG can for example find such edges. The advantage of sparse and semi dense methods is that they are less computationally expensive than dense methods.\\\\
Sparse direct methods don’t create dense clouds immediately however they can work on weaker keypoints than indirect methods. Therefore, they can create denser clouds than e.g. ORB SLAM. They can also be computational less expensive than indirect methods because they don’t have to calculate expensive features \cite{svo}.

\section{Stereo and Monocular SLAM}

In this project, the focus is on stereo SLAM. A lot of today's paper focus on monocular SLAM. The reasons are that monocular cameras are cheaper and monocular cameras are available in Smartphones. Monocular SLAM has two problems we don’t have with stereo SLAM. First an initialization process has to estimate the movement between two frames. We can only create a 3D point cloud based on two images with known position, in the monocular case the position of two images is random. Therefore, the algorithm needs to guess the pose and transformation over a few images until it knows the poses. It does that by making assumptions on the world or by trying to optimizing the point positions over several images. However, even when successfully initialized the second problem persists. Monocular SLAM is not able to guess the scale of the point cloud. The intuition for this issue is that we can’t distinguish between the movement of the camera on a small model close to the scene or the movement of the camera on the original model further away from the scene. With stereo SLAM we don’t have this issue. For each camera pose we get two images with a known distance between the two camera sensors (baseline). Therefore, we can calculate the depth of each pixel in real world distance. From this we can generate a point cloud assuming that the initial camera pose is at origin. We therefore have an instantaneous initialization and can calculate the real world position in a known unit e.g. meters. Tracking between two camera poses is however the same on monocular and stereo SLAM. However we can again insert new keypoints immediately on stereo cameras without the need of using a second camera pose.

\section{Decision}

In our previous project works we used ORB SLAM for pose estimation and SLAM. While ORB SLAM is powerful it has some performance limitations when running on embedded systems. The CPVR lab uses ORB SLAM heavily in different projects but there is no experience with direct or semi direct approaches yet. Therefore, we try to reimplement SVO SLAM in this thesis. It should have less dependencies and run faster than the open source version available on Github. The decision for SVO SLAM was taken because of:
\begin{itemize}
	\item Open source implementation available
	\item According to SVO paper it's faster than ORB SLAM
	\item Approach seems to be ``simpler'' than ORB SLAM
	\item The CPVR lab already has experience with ORB, an alternative solution may be interesting.
\end{itemize}

\chapter{Camera Evaluation}

To do stereo computer vision, we need a stereo camera. There are a few stereo cameras available on the market. It is also possible to build a stereo camera with two monocular cameras. However, we decided against that because it would require a mechanism to synchronize the images from both streams. We analyze three stereo cameras available on the market.

\section{ZED}
The ZED camera \cite{zed} is a stereo camera from Stereo Labs. Here the specification of this camera:
\begin{itemize}
	\item Full HD color images with 30fps
	\item USB3.0
	\item UVC compliant
	\item Linux SDK
	\item Baseline 120mm
	\item IMU sensor with 6DoF
	\item Price: 449\$
\end{itemize}

To run their SDK a Dual Core CPU with 2.3 GHz and CUDA > 3.0 is required. However, it's also possible to use the camera without their SDK.

\section{ECON Tara}
The Tara camera \cite{tara} from Econ is another stereo camera. In comparison to the ZED camera it delivers a reduced resolution and only gray scale images.
\begin{itemize}
	\item 752x480 gray scale images with 60fps
	\item USB3.0
	\item Global Shutter Camera
	\item UVC compliant
	\item Baseline 60mm
	\item IMU sensor with 6DoF
	\item Open Source SDK
	\item Price: 149\$
\end{itemize}

It has a small OpenCV based SDK with open source software. In comparison to the ZED SDK it is less powerful.

\section{Intel RealSense D435}
The Intel RealSense camera \cite{realsense} is a infrared stereo camera. In comparison to RGB stereo cameras it uses infrared images for stereo vision. It seems that this sensors are less prone to reflections. A third camera delivers RGB images in full HD.
\begin{itemize}
	\item 1280x720 with 90fps (IR), 1920x1080 with 30fps (RGB)
	\item USB3.0
	\item Global Shutter Camera
	\item UVC compilant (kernel patch required)
	\item Baseline 50mm
	\item Open Source SDK (librealsense)
	\item Price: 179\$
\end{itemize}

The Intel SDK seems quite powerful and is open source as well. However, the indepth details of the camera are not publicly available.

\section{Decision}

We decided to use the Econ Tara because it has an outstanding price ratio and because it is a conventional stereo camera. However, the Intel RealSense camera is also a great fit because it delivers depth images directly. This reduces the CPU load on the iMX8. If we require Full HD, we should use the ZED camera. Another advantage of the ZED camera is the bigger baseline of 120mm. This allows depth estimation with higher range. We decided against this camera mainly because of the high price and because of the closed source SDK. We can't benefit from Full HD sensors because embedded systems only have limited CPU resources.



\chapter{SVO}
In this chapter we describe SVO SLAM.

\section{SVO SLAM idea}
As described in chapter \ref{chap:evaluation} there are two main ideas on how SLAM algorithms can estimate the current pose. SVO is part of the sparse direct algorithms. This means it tries to update the current camera pose based on intensity differences. To make it fast it compares only keypoints that were extracted earlier. This makes the algorithm faster and more efficient.

\subsection{Initialization}\label{sec:initialization}
In this project we use a stereo camera therefore the initialization step is simpler than in the monocular case. The algorithm performs the following steps.
\begin{enumerate}
	\item{Search FAST keypoints on the whole image}
	\item{Do Sobel filtering in horizontal direction}
	\item{Divide image in 16x16 blocks}
	\item{Select one FAST corner in each block}
		\subitem{If no FAST corner can be found, use point with highest gradient as keypoint}
	\item{Calculate depth fore each keypoint}
		\subitem{Find the smallest intensity difference along the epipolar line}
		\subitem{The difference between these two points is the disparity (1/depth)}
\end{enumerate}

This step is different compared to what SVO SLAM describes in \cite{svo2} section IX.A. The reason is that we focus on stereo cameras. Therefore, we can already estimate a point cloud from the first image without any movement. We calculate the 3D points for each keyframe. However, we try to keep as many kepoints from previous keyframes as possible to make the algorithm more robust.

\subsection{Sparse Image Alignment}\label{sec:sia}

From the initialization step we optain an initial point cloud. We use this point cloud to estimate the pose and motion of the camera. We try to find a Pose that minimizes the photometric error of small patches around the keypoints found in section \ref{sec:initialization}. We do that by minimizing the formula shown in equation \ref{eq:intensity}. We use equation \ref{eq:cm} to project the 3D point cloud to the current frame. By using Levenberg-Marquardt we can minimize the intensity difference by changing the values of the extrinsic matrix $r_{ij}$ and $t_{x,y,z}$. As initial guess for the pose we use the current pose and add the motion model. The motion model is the difference between the pose in the previous frame ($t-1$) and frame $t-2$.

\begin{equation}\label{eq:intensity}
	I_{TD}=\sum_{i=0}^{i=N}I_{F-1}(x_{F-1,i},y_{F-1,i})-I_{F}(x_{F,i},y_{F,i})
\end{equation}
Where:
\begin{align*}
	I_{TD} &:					\text{Total intensity difference over all keypoints}\\
	I_{F-1} &:				\text{Previous grayscale image}\\
	I_{F} &:					\text{Current grayscale image}\\
	x_{KF},y_{KF} &:	\text{x,y position in keyframe}\\
	N &:							\text{Total number of keypoints}\\
	x_{F},y_{F} &:		\text{x,y posiiton in current frame}
\end{align*}

\begin{equation}\label{eq:cm}
  \begin{pmatrix}
		f_x & \gamma & c_x \\
		0 & f_y & c_y \\
		0 & 0 & 1 \\
	\end{pmatrix}*
	\begin{pmatrix}
		r_{11} & r_{12} & r_{13} & t_x \\
		r_{21} & r_{22} & r_{23} & t_y \\
		r_{31} & r_{32} & r_{33} & t_z \\
	\end{pmatrix}
	\begin{pmatrix}
		X \\
		Y \\
		Z \\
		1
	\end{pmatrix}=
	\begin{pmatrix}
		u \\
		v \\
		s
  \end{pmatrix}
\end{equation}
\begin{equation}\label{eq:cm_normalized}
	\begin{pmatrix}
		x \\
		y
	\end{pmatrix}=
	\begin{pmatrix}
		u/s \\
		v/s 
  \end{pmatrix}
\end{equation}
Where:
\begin{align*}
  X,Y,Z			&: \text{point in the 3D world}\\
	u,v,s	   	&: \text{point in 2D image not normalize}\\
	x,y				&: \text{point in 2D image normalized with s}\\
	f_x,f_y  	&: \text{focal length of the camera}\\
  c_x,c_y  	&: \text{principal point}\\
  t_x,t_y,t_z	&: \text{location of the camera}\\
  r_{ij}	&: \text{part of the rotation matrix}
\end{align*}

This step is described in \cite{svo2} in section IV.A. We get a first estimate for the pose. However, because we estimate the current pose by using the previous frame we will get a drift in the long term. Therefore, we need to refine the pose by using the keyframe as reference.\\
If less than 80\% of the keypoints are trackable we need to insert a new keyframe based on the previous frame. In comparison SVO inserts a new keyframe if the movement is more than 12\% of the average depth. In a first step we try to do it as described, however maybe this needs to change in the future. See \cite{svo2} section X.D for the implementation details.

\subsection{Refinement}\label{sec:refinement}

The refinement step is necessary to reduce the drift over time by taking the information of the keyframe into account. We use the Lucas-Kanade algorithm to find an Affine transformation Matrix that minimizes the photometric error between all keypoints of the keyframe and the current frame. This will give us a reprojection error. We try to minimize this reprojection error by do ing a bundle adjustment over the pose (motion only) and over the 3D point cloud (structure only).\\

Lucas-Kanade tries to find the movement of points by trying to find a movement (dx, dy) that minimizes the intensity difference as shown in equation \ref{eq:lucaskanade}
\begin{equation}\label{eq:lucaskanade}
	I(x,y,t)=I(x+dx, y+dy, t+dt)
\end{equation}
Where:
\begin{align*}
	I(x,y,t)						&:	\text{intensity in template}\\
	I(x+dx, y+dy, t+dt) &:	\text{intensity in new image t+dt, moved by dx and dy}
\end{align*}

Lucas Kanade tries to solve this equation by assuming that the intensity gradient at a keypoint will point in the direction of the movement \cite{lucas_kanade}. The SVO paper doesn't use Lucas Kanade to do the 2d point adjustment. They use the Inverse Compositional method instead. In comparison to Lucas Kanade they will find one Affine Warp matrix that describes the movement of all points. Lucas Kanade as implemented in OpenCV will move each point separate. However, because no Inverse Compositional method is available in OpenCV Lucas Kanade was used as a first implementation. This will be less efficient than using the Inverse Compositional method.

\subsubsection{Pose refinement}
In section \ref{sec:sia} we estimated the pose of the current frame by using the 3D point cloud and the previous image. Based on the estimated pose we then refined the 2D point position as described in section \ref{sec:refinement}. Now we need to update the estimated pose in regards to the new 2D point position. This is done by minimizing the reprojection error when changing the pose. We use Levenberg-Marquardt to minimize the pose in this step.
\begin{equation}\label{eq:pose_refinement}
	min((x - x')^2 + (y -y')^2)
\end{equation}
Where:
\begin{align*}
	x,y		&: \text{x,y position in image after refinement}\\
	x',y' &: \text{x,y reprojection of 3D point by using \ref{eq:cm}}
\end{align*}

\subsection{Point refinement}
Because of the stereo camera approach we use in this work it's not necessary to follow the same mapping as described in \cite{svo2}. However, specially the depth estimation of points farer away is not accurate because we have a maximum resolution of 1 pixel. When moving the camera along the x axis we could gain higher precision because this is comparable to a higher baseline. We therefore use a Kalman filter to fuse depth estimations from different poses. This is efficient and can therefore be done for each frame. 

\section{Optical Flow}
To speed up the optimization a predefined gradient can be calculated. The method to find such a gradient is related to the Lucas-Kanade optical flow method. Instead of searching a Warp matrix we search a transformation matrix in this scenario. We have two images $I_{k-1}$ and $I_{k}$. We know the pose of image k-1 $T_{k-1}$ . We search the pose $T_k$.\\
While the inverse compositional Lucas Kanade algorithm tries to find the optimal Warp matrix. We try to find the optimal pose.
\subsection{Lucas Kanade Intuitive}

As mentioned in the previous section Lucas Kanade is an algorithm to estimate the optical flow. In the simplest case it just tells us where a certain pixel was in the image before and where it is now. It does that in a efficient way by using the intensity gradients. This section tries to describe how this works in an intuitive but simplified way while the next section will describe it in a more mathematical way.\\
Let's assume the simplest scenario where we can only move in one direction (left or right). Figure \ref{fig:optical_flow_intuitive} shows this simple scenario. Let's first focus on the case where we have a small movement (a). We first calculate the intensity gradients in the template (first row). Then we calculate the intensity difference (last row) between the current image (second row) and the first image. Now we multiply the gradient times the intensity difference this will give us the direction in which we have to move. Here we assume that moving the current image to left means - and moving it to right +. Now we move in the direction of the gradient and will calculate the intensity difference again. If it increases we moved too much and we have to reduce the step size if it decreases we are fine and we can calculate the new gradient again based on the new intensity difference. After some iterations the intensity change from one iteration to the other should be minimal. We have found the horizontal movement of our image. We already describe the inverse compositional Lucas Kanade algorithm because we calculate the gradients on the template. The ``normal'' Lucas Kanade would calculate the gradients on the current image. This has the disadvantage that we need to calculate the gradients after each iteration and it's therefore less computational efficient.\\
What we can see in \ref{fig:optical_flow_intuitive} (b) that it's impossible to track movements over bigger distances. If the patch we compare does not connect to the area where the new image has moved, we are unable to move in the right direction. As we can see it's even possible that we move in the wrong direction (b). To overcome this problem we can downsample the image as shown in (c). We only downsampled in horizontal direction for visability. In reality the whole image would be downsampled by factor 2,4,8,16, etc. This is called an optical pyramid. As we can see in (c) after we downsampled the image by factor two we have again a valid gradient which points in the right direction. The idea is that we start to optimized in the lowest level if we converge we optimize again one level higher and so on. With this idea it is possible to track flows over higher distances while still being efficient.

\begin{figure}[H]
	\centering
	\subfloat[Lucas Kanade small move]{\includegraphics[height=0.2\textheight]{img/optical_flow_intuitive_1.png}}
	\subfloat[Lucas Kanade big move]{\includegraphics[height=0.2\textheight]{img/optical_flow_intuitive_2.png}}
	\subfloat[Lucas Kanade big move down sampled]{\includegraphics[height=0.2\textheight]{img/optical_flow_intuitive_3.png}}
	\caption{Optical Flow Intuitive}\label{fig:optical_flow_intuitive}
\end{figure}

We can extend the above scenario from a movement in one dimension easily to two dimensions as shown in figure \ref{fig:optical_flow_2d}. We simply calculate the gradient in the vertical direction as well. By doing this we can track movements in x and y direction. We easily see that not all values in the patch help to find the right gradient. Some gradients in the patch point in the wrong direction. However, in total the gradient points in the right direction. By using image pyramids we can reduce the influence of such miss matches.
\begin{figure}[H]
	\includegraphics[width=1.0\textwidth]{img/optical_flow_2d.png}
	\caption{Optical Flow Intuitive 2D: +: current-previous intensity > 0, -: current-previous intensity < 0, u: gradient points up, d: gradient points down, l: gradient points left, r: gradient points right, red: wrong direction, green: right direction, blue: no influence, yellow arrow: final gradient}\label{fig:optical_flow_2d}
\end{figure}

We showed that optical flow can track two dimensional movements. However, it doesn't track rotations and shearing. This will introduce dependencies between x and y movements which become non-linear. In the next section we extend the intuitive approach so that we can solve non-linear problems.

\subsection{Lucas Kanade Mathematical}

In the previous section we showed the basic idea of optical flow. It is oversimplified and can only track movements in x and y direction. In this section we extend this idea so that more complicated movements can be tracked.\\

Instead of linear movements in x and y direction we want to find a warp matrix which describes a affine transformation of a patch from one image to the other (equation \ref{eq:lk_warp}).
\begin{equation}\label{eq:lk_warp}
	p=\begin{pmatrix}
		p_{11} & p_{12} & p_{13} \\
		p_{21} & p_{22} & p_{23}
	\end{pmatrix}
\end{equation}
\begin{align*}
	p				&:	\text{warp matrix}\\
	p_{ij}	&:	\text{parameter of warp matrix}
\end{align*}
This warp matrix describes movements along the x and y axis($p_{11},p_{22}$, a rotation ($p_{13},p_{21}$) and shearing ($p_{13},p_{23}$). The goal of Lucas Kanade is to find this warp matrix. It does that by minimizing the Intensity difference between patch of pixels in a template (reference image) and a patch of pixels with the same size in the current image as shown in equation \ref{eq:lk_problem}. 

\begin{equation}\label{eq:lk_problem}
	\sum_x\sum_y(I_{k}(x',y')-I_{k-1}(x,y))^2
\end{equation}
\begin{align*}
	x,y				&:	\text{Pixel position in template}\\
	x',y'			&:	\text{Pixel position in current image}\\
	I_{k-1}		&:	\text{Template image}\\
	I_{k}			&:	\text{Current image}
\end{align*}

We describe the transformation of a pixel from the reference image to the current image as a matrix multiplication as shown in equation \ref{eq:lk_warped}.

\begin{equation}\label{eq:lk_warped}
	W=
	\begin{pmatrix}
		x' \\
		y'
	\end{pmatrix}=
	\begin{pmatrix}
		p_{11} & p_{12} & p_{13} \\
		p_{21} & p_{22} & p_{23}
	\end{pmatrix}*
	\begin{pmatrix}
		x\\
		y\\
		1
	\end{pmatrix}=
	\begin{pmatrix}
		p_{11}*x + p_{12}*y + p_{13} \\
		p_{21}*x + p_{22}*y + p_{23}
	\end{pmatrix}
\end{equation}
\begin{align*}
	W					&:	\text{Warp}\\
	x',y'			&:	\text{Pixel position in current image}\\
	x,y				&:	\text{Pixel position in template}\\
	p_{ij}		&:	\text{parameter of warp matrix}
\end{align*}

Because \ref{eq:lk_problem} is a nonlinear problem we have to solve it with a nonlinear solver (e.g. Gauss-Newton or Gradient Descent). The problem is solved by iteratively changing the warp matrix in the direction of a gradient until the difference of the intensities are minimal:
\begin{equation}
	\begin{pmatrix}
		x' \\
		y'
	\end{pmatrix}=
	\begin{pmatrix}
		p_{11}+\Delta p_{11} & p_{12}+\Delta p_{12} & p_{13}+\Delta p_{13} \\
		p_{21}+\Delta p_{21} & p_{22}+\Delta p_{22} & p_{23}+\Delta p_{23}
	\end{pmatrix}*
	\begin{pmatrix}
		x\\
		y\\
		1
	\end{pmatrix}
\end{equation}
\begin{align*}
	x',y'					&:	\text{Pixel position in current image}\\
	x,y						&:	\text{Pixel position in template}\\
	p_{ij}				&:	\text{parameter of warp matrix}\\
	\Delta p_{ij}	&:	\text{change of $p_{ij}$ per iteration}
\end{align*}


After each iteration we set $p=p_{i-1}+\Delta p$. We can solve this problem by approximating the gradient heuristically. However, it would slow down the calculation. Therefore, Lucas Kanade calculates the gradient by doing a first order Taylor approximation of the problem:
\begin{equation}\label{eq:lk_taylor}
	\sum_x\sum_y(I_{k}(x_{i-1}',y_{i-1}')+\nabla I_{k}*\frac{\sigma W}{\sigma p}*\Delta p-I_{k-1}(x,y))^2
\end{equation}
\begin{align*}
	x,y				&:	\text{Pixel position in template}\\
	x',y'			&:	\text{Pixel position in current image}\\
	I_{k-1}		&:	\text{Template image}\\
	I_{k}			&:	\text{Current image}\\
	\nabla I	&:	\text{Gradient in image I at position x',y'}
\end{align*}
We want to find $\Delta p$ which is our gradient. To make this work we have to derive equation \ref{eq:lk_taylor}. This gives us:
\begin{equation}
	2*\sum_x\sum_y\begin{bmatrix}\nabla I_{k}*\frac{\sigma W}{\sigma p}\end{bmatrix}^T*\begin{bmatrix}I_{k}(x_{i-1}',y_{i-1}')+\nabla I_{k}*\frac{\sigma W}{\sigma p}*\Delta p-I_{k-1}(x,y)\end{bmatrix}
\end{equation}
We can now find $\Delta p$ by moving it to the left:
\begin{equation}\label{eq:lk_dp}
	\Delta p=(\sum_x\sum_y\begin{bmatrix}\nabla I_{k}*\frac{\sigma W}{\sigma p}\end{bmatrix}^T*\begin{bmatrix}\nabla I_{k}*\frac{\sigma W}{\sigma p}\end{bmatrix})^{-1}
	*\sum_x\sum_y\begin{bmatrix}\nabla I_{k}*\frac{\sigma W}{\sigma p}\end{bmatrix}^T*\begin{bmatrix}I_{k-1}(x,y) - I_{k}(x_{i-1}',y_{i-1}'\end{bmatrix}
\end{equation}

$\frac{\sigma W}{\sigma p}$ is the Jacobian matrix and looks as follows:
\begin{equation}
	\frac{\sigma W}{\sigma p}=
	\begin{pmatrix}
		x & y & 1 & 0 & 0 & 0 \\
		0 & 0 & 0 & x & y & 1
	\end{pmatrix}
\end{equation}

All parameters in equation \ref{eq:lk_dp} are known and can be calculated for a given x,y and p. However, the following part of equation \ref{eq:lk_dp} is time consuming to calculate and has to be inverted:
\begin{equation}
	\Delta H=(\sum_x\sum_y\begin{bmatrix}\nabla I_{k}*\frac{\sigma W}{\sigma p}\end{bmatrix}^T*\begin{bmatrix}\nabla I_{k}*\frac{\sigma W}{\sigma p}\end{bmatrix})
\end{equation}
H is called the Hessian matrix. Unfortunately we need to calculate this matrix in each iteration. The inverse compositional Lucas Kanade algorithm tries to solve this problem by wrapping the template to the current image instead of the current image to the template.

\subsection{Inverse compositional Lucas Kanade}

The inverse compositional Lucase Kanade algorithm does more or less the same as the normal Lucas Kanade algorithm but it searches the inverse wrap update at each iteration. The problem basically stays the same but we switch the role of the template with the one of the image which first doesn't have any effect:
\begin{equation}\label{eq:iclk_problem}
	\sum_x\sum_y(I_{k-1}(x'',y'')-I_{k}(x',y'))^2
\end{equation}
\begin{align*}
	x'',y''		&:	\text{Warped pixel position in the template}\\
	x',y'			&:	\text{Pixel position in the current image}\\
	I_{k-1}		&:	\text{Template image}\\
	I_{k}			&:	\text{Current image}
\end{align*}

What is different this time is that we wrap the template with $\Delta p$:
\begin{equation}
	\begin{pmatrix}
		x'' \\
		y''
	\end{pmatrix}=
	\begin{pmatrix}
		\Delta p_{11} & \Delta p_{12} & \Delta p_{13} \\
		\Delta p_{21} & \Delta p_{22} & \Delta p_{23}
	\end{pmatrix}*
	\begin{pmatrix}
		x\\
		y\\
		1
	\end{pmatrix}
\end{equation}
\begin{align*}
	x,y						&:	\text{Original pixel position in template}\\
	x'',y''				&:	\text{Warped pixel position in template}\\
	\Delta p_{ij}	&:	\text{change of $p_{ij}$ per iteration}
\end{align*}

\begin{equation}
	\begin{pmatrix}
		x' \\
		y'
	\end{pmatrix}=
	\begin{pmatrix}
		p_{11} & p_{12} & p_{13} \\
		p_{21} & p_{22} & p_{23}
	\end{pmatrix}*
	\begin{pmatrix}
		x\\
		y\\
		1
	\end{pmatrix}
\end{equation}
\begin{align*}
	x',y'					&:	\text{Pixel position in current image}\\
	x,y						&:	\text{Original pixel position in template}\\
	p_{ij}				&:	\text{parameter of warp matrix}\\
\end{align*}

With this we separate the update step from the wrapping of the image. The update step will then be $p=p_{i-1}+(\Delta p)^{-1}$. If we do the first order taylor approximation again we get:

\begin{equation}\label{eq:iclk_taylor}
	\sum_x\sum_y(I_{k-1}(x,y)+\nabla I_{k-1}*\frac{\sigma W}{\sigma p}*\Delta p-I_{k}(x',y'))^2
\end{equation}
\begin{align*}
	x,y				&:	\text{Original pixel position in template}\\
	x',y'			&:	\text{Pixel position in current image}\\
	I_{k-1}		&:	\text{Template image}\\
	I_{k}			&:	\text{Current image}\\
	\nabla I	&:	\text{Gradient in image I at position x',y'}
\end{align*}

This gives us for $\Delta p$:
\begin{equation}\label{eq:iclk_dp}
	\Delta p=(\sum_x\sum_y\begin{bmatrix}\nabla I_{k-1}*\frac{\sigma W}{\sigma p}\end{bmatrix}^T*\begin{bmatrix}\nabla I_{k-1}*\frac{\sigma W}{\sigma p}\end{bmatrix})^{-1}
	*\sum_x\sum_y\begin{bmatrix}\nabla I_{k-1}*\frac{\sigma W}{\sigma p}\end{bmatrix}^T*\begin{bmatrix}I_{k}(x_{i-1}',y_{i-1}') - I_{k-1}(x,y)\end{bmatrix}
\end{equation}

Compared to the normal Lucas-Kanade algorithm the Hessian Matrix H is now constant for all iterations and we don't have to do the expensive calculation at every iteration:
\begin{equation}
	\Delta H=\sum_x\sum_y\begin{bmatrix}\nabla I_{k-1}*\frac{\sigma W}{\sigma p}\end{bmatrix}^T*\begin{bmatrix}\nabla I_{k-1}*\frac{\sigma W}{\sigma p}\end{bmatrix}
\end{equation}

Because we only have to calculate the Hessian matrix once, the inverse computational Lucas Kanade is more efficient then the Lucas Kanade algorithm.

\subsection{Pose estimation}
For a first estimate of the pose we use some kind of Lucas Kanade algorithm. However, we don't have the point position in the new image directly but we know the 3d position of the point. We now optimize the projection matrix so that the intensity difference is minimized again. So instead of having equation \ref{eq:lk_warped} we have:

\begin{equation}\label{eq:pe_warped}
	\begin{pmatrix}
		u \\
		v \\
		s
	\end{pmatrix}=
	\begin{pmatrix}
		r_{11} & r_{12} & r_{13} & t_{1} \\
		r_{21} & r_{22} & r_{23} & t_{2} \\
		r_{31} & r_{32} & r_{33} & t_{3}
	\end{pmatrix}*
	\begin{pmatrix}
		X\\
		Y\\
		Z
	\end{pmatrix}=
	\begin{pmatrix}
		r_{11}*X + r_{12}*Y + r_{13}*Z + t_{1} \\
		r_{21}*X + r_{22}*Y + r_{23}*Z + t_{2} \\
		r_{31}*X + r_{32}*Y + r_{33}*Z + t_{3} \\
	\end{pmatrix}
\end{equation}
\begin{equation}
	\begin{pmatrix}
		x \\
		y 
	\end{pmatrix}=
	\begin{pmatrix}
		\frac{u}{s} \\
		\frac{v}{s} 
	\end{pmatrix}=
	\begin{pmatrix}
		\frac{r_{11}*X + r_{12}*Y + r_{13}*Z + t_{1}}{r_{31}*X + r_{32}*Y + r_{33}*Z + t_{3}}  \\
		\frac{r_{21}*X + r_{22}*Y + r_{23}*Z + t_{2}}{r_{31}*X + r_{32}*Y + r_{33}*Z + t_{3}}
	\end{pmatrix}
\end{equation}

\begin{align*}
	u,v,s			&:	\text{Pixel position scaled with s}\\
	X,Y,Z			&:	\text{3D Pixel position}\\
	r_{ij}		&:	\text{Rotation matrix}\\
	t_{i}			&:	\text{translation vector}\\
\end{align*}

We would need to find 12 gradients if we use the formulation in equation \ref{eq:pe_optimization} for the inverse compositional Lucas Kanade. However, we could express the $r_{ij}$ components based on roll, pitch and yaw which are our 6 DoF. It is easy to see that this formulation gets rather complex and it's nearly impossible to find the Jacobian for such a matrix.
\begin{equation}\label{eq:pe_optimization}
	\begin{pmatrix}
		x'' \\
		y''
	\end{pmatrix}=
	\begin{pmatrix}
		\frac{\Delta r_{11}*X + \Delta r_{12}*Y + \Delta r_{13}*Z + \Delta t_{1}}{ \Delta r_{31}*X + \Delta r_{32}*Y + \Delta r_{33}*Z + \Delta t_{3}}  \\
		\frac{\Delta r_{21}*X + \Delta r_{22}*Y + \Delta r_{23}*Z + \Delta t_{2}}{ \Delta r_{31}*X + \Delta r_{32}*Y + \Delta r_{33}*Z + \Delta t_{3}}
	\end{pmatrix}
	\begin{pmatrix}
		x\\
		y\\
		1
	\end{pmatrix}
\end{equation}
\begin{align*}
	x,y						&:	\text{Original pixel position in template}\\
	x'',y''				&:	\text{Warped pixel position in template}\\
	\Delta p_{ij}	&:	\text{change of $p_{ij}$ per iteration}
\end{align*}

To make finding the Jacobian for this matrix possible we use Lie algebra. The whole idea is to transform the angles and translations into another space where it's easier to find the Jacobian. The optimization is then done with Lie algebra. The result from the optimization needs to be transformed from Lie algebra back to a Lie group which will give us pose and angles. The problem we need to solve stays the same as in equation \ref{eq:iclk_problem}. However we need to find $x''$ and $y''$ differently. We use the following formulation:
\begin{equation}
	W=e^{\xi}*P*p
\end{equation}
\begin{align*}
	e			&:	\text{Exponential map}\\
	\xi		&:	\text{Pose change between reference frame and current frame as SE(3) vector}\\
	P			&:	\text{Pose of the reference frame}\\
	p			&:	\text{point in 3D cloud}
\end{align*}

We want to find $\xi$ so that the intensity difference gets minimal. $P*p$ can be calculated and gives us the following Jacobian matrix for $\frac{W}{\delta \xi}$ (see \cite{se3_explain}).
\begin{equation}
	\frac{e^{\xi}*P*p}{\delta \xi}=\begin{pmatrix}
		\frac{1}{z} & 0 & -\frac{x}{z^2} & -\frac{x*y}{z^2} & 1 + \frac{x^2}{z^2} & -\frac{y}{z} \\
		0 & \frac{1}{z}  & -\frac{y}{z^2} & -1 - \frac{y^2}{z^2} & \frac{x*y}{z^2} &  \frac{x}{z}
	\end{pmatrix}
\end{equation}



\subsubsection{3D cloud refinement}
After refining the pose we still have a small reprojection error. We can try to minimize this error by adjusting the 3D position for each point. Here we do the same thing as in equation \ref{eq:pose_refinement} but we now change the 3D point position instead of the pose. Because we need to do that for all point this is more computational expensive than the previous step. Instead of Levenberg-Marquardt we use normal Gradient-Descent in this step, because the here described problem is an underdeterminated problem. However, we can still find a local minimum. This step follows section V.B in \cite{svo2}. However, pose and 3D cloud is only updated for the last camera frame. We do not bundle adjust the whole trajectory.

\subsection{Mapping}

Mapping can be solved in a separate thread. The mapping thread decides if a new keyframe must be insert. If no keyframe is required, we can use the current frame to update the depth of the 3D cloud by adjusting the inverse depth with a Kalman Filter. Mapping is not solved yet.

\begin{thebibliography}{1}

	\bibitem{svo}
	Christian Forster et al\\
	\textit{SVO: Semi-Direct Visual Odometry for Monocular and Multi-Camera Systems}\\
	doi:10.1109/TRO.2016.2623335

	\bibitem{svo2}
	Christian Forster et al\\
	\textit{SVO: Semi-Direct Visual Odometry for Monocular and Multi-Camera Systems}\\
	doi:10.1109/TRO.2016.2623335

	\bibitem{lucas_kandae}
	OpenCV\\
	\textit{Optical Flow}\\
	https://docs.opencv.org/4.0.1/d7/d8b/tutorial\_py\_lucas\_kanade.html


	\bibitem{inverse_compositional}
	S. Baker and I. Matthews\\
	\textit{Lucas-kanade 20 years on: A unifying framework}
	Int. J. Comput. Vis., 56(3):221–255, 2004.


	\bibitem{se3_explain}
	José Luis Blanco Claraco\\
	\textit{A tutorial on SE(3) transformation parameterizations and on-manifold optimization José Luis Blanco Claraco}
	https://w3.ual.es/personal/jlblanco/, Last update 18/03/2019



\end{thebibliography}

\end{document}
